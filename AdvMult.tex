\chapter{Advanced Multivariate Methods}
\label{advmul}

\section{Conditional Distributions}

The key to good probability modeling and statistical analysis is to
understand conditional probability.  The issue arises constantly.

\subsection{Conditional Pmfs and Densities}

First, let's review:  In many repetitions of our ``experiment,'' P(A) 
is the long-run proportion of the time that A occurs.  By contrast,
P(A$|$B) is the long-run proportion of the time that A occurs, {\it
among those repetitions in which B occurs.}  Keep this in your mind at
all times.

Now we apply this to pmfs, densities, etc.  We define the conditional
pmf as follows for discrete random variables X and Y:

\begin{equation}
\label{discrcond}
p_{Y|X}(j|i) = P(Y = j | X = i) = \frac{p_{X,Y}(i,j)}{p_X(i)}
\end{equation}

By analogy, we define the conditional density for continuous X and Y:

\begin{equation}
\label{contcond}
f_{Y|X}(t|s) = \frac{f_{X,Y}(s,t)}{f_X(s)}
\end{equation}

\subsection{Conditional Expectation}

Conditional expectations are defined as straightforward extensions of
(\ref{discrcond}) and (\ref{contcond}):

\begin{equation}
E(Y|X=i) = \sum_{j} j p_{Y|X}(j|i) 
\end{equation}

\begin{equation}
E(Y | X = s) = \int_{t} t f_{Y|X}(t|s) ~ dt
\end{equation}

\subsection{The Law of Total Expectation (advanced topic)}
\label{lte}

\subsubsection{Conditional Expected Value As a Random Variable}

For a random variable Y and an event A, the quantity E(Y$|$A) is the
long-run average of Y, among the times when A occurs.  Note several
things about the expression E(Y$|$A):

\begin{itemize}

\item The item to the left of the $|$ symbol is a {\it random variable} (Y).

\item The item on the right of the $|$ symbol is an {\it event} (A).

\item The overall expression evaluates to a constant.

\end{itemize}

By contrast, for the quantity E(Y$|$W) to be defined shortly for a random
variable W, it is the case that:

\begin{itemize}

\item The item to the left of the $|$ symbol is a random variable (Y).

\item The item to the right of the $|$ symbol is a random variable (W).

\item The overall expression itself is a random variable, not a constant.

\end{itemize}

It will be very important to keep these differences in mind.

Consider the function g(t) defined as\footnote{Of course, the t is just
a placeholder, and any other letter could be used.} 

\begin{equation}
\label{thisisg}
g(t)=E(Y|W=t)
\end{equation}

In this case, the item to the right of the $|$ is an event, and thus
g(t) is a constant (for each value of t), not a random variable.

\begin{definition}

Define g() as in (\ref{thisisg}).  Form the new random variable Q =
g(W).  Then the quantity E(Y$|$W) is defined to be Q. 

\end{definition}

(Before reading any further, re-read the two sets of bulleted items
above, and make sure you understand the difference between E(Y$|$W=t)
and E(Y$|$W).)

One can view E(Y$|$W) as a projection in an abstract vector space.  This
is very elegant, and actually aids the intuition.  If (and only if) you
are mathematically adventurous, read the details in Section
\ref{elegant}.

\subsubsection{Famous Formula: Theorem of Total Expectation}

An extremely useful formula, given only scant or no mention in
most undergraduate probability courses, is 

\begin{equation}
\label{itex}
E(Y)=E[E(Y|W)]
\end{equation}

for any random variables Y and W (for which the expectations are
defined).  

The RHS of (\ref{itex}) looks odd at first, but it's merely E[g(W)];
since Q =  E(Y$|$W) is a random variable, we can certainly ask what its
expected value is.

Equation (\ref{itex}) is a bit abstract.  It's a very useful
abstraction, enabling streamlined writing and thinking about the
probabilistic structures at hand.  Still, you may find it helpful to
consider the case of discrete W, in which (\ref{itex}) has the more
concrete form

\begin{equation}
\label{itexdiscrete}
EY = \sum_{i} P(W=i) \cdot E(Y|W=i)
\end{equation}

To see this intuitively, think of measuring the heights and weights of
all the adults in Davis.  Say we measure height to the nearest inch, so
that height is discrete.  We look at all the adults in Davis who are 72
inches tall, and write down their mean weight.  Then we write down the
mean weight of all adults of height 68.  Then we write down the mean
weight of all adults of height 75, and so on.  Then (\ref{itex}) says
that if we take the average of all the numbers we write down---the
average of the averages---then we get the mean weight among {\it all}
adults in Davis.  

Note carefully, though, that this is a {\it weighted} average.  If for
instance people of height 69 inches are more numerous in the population,
then their mean weight will receive greater emphasis in over average of
all the means we've written down.  This is seen in (\ref{itexdiscrete}),
with the weights being the quantities P(W=i).

The relation (\ref{itex}) is proved in the discrete case in Section
\ref{proveitex}.

\subsection{What About the Variance?}
\label{totalvar}

By the way, one might guess that the analog of the Theorem of Total Expectation
for variance is

\begin{equation}
Var(Y)=E[Var(Y|W)]
\end{equation}

\textit{But this is false.} Think for example of the extreme case in which Y
= W. Then Var(Y$|$W) would be 0, but Var(Y) would be nonzero.

The correct formula, called the Law of Total Variance, is

\begin{equation}
\label{bis}
Var(Y)=E[Var(Y|W)]+Var[E(Y|W)]
\end{equation}

Deriving this formula is easy, by simply evaluating both sides of
(\ref{bis}), and using the relation $Var(X) = E(X^2) -(EX)^2$. This
exercise is left to the reader.  See also Section \ref{elegant}.

\subsection{Example:  Trapped Miner}
\label{trappedminer}

(Adapted from \textit{Stochastic Processes,} by Sheldon Ross, Wiley, 1996.)

A miner is trapped in a mine, and has a choice of three doors.  Though
he doesn't realize it, if he chooses to exit the first door, it will
take him to safety after 2 hours of travel.  If he chooses the second
one, it will lead back to the mine after 3 hours of travel. The third
one leads back to the mine after 5 hours of travel. Suppose the doors
look identical, and if he returns to the mine he does not remember which
door(s) he tried earlier. What is the expected time until he reaches
safety?

Let Y be the time it takes to reach safety, and let W denote the number
of the door chosen (1, 2 or 3) on the first try. Then let us consider
what values E(Y$|$W) can have. If W = 1, then Y = 2, so

\begin{equation}
E(Y|W=1)=2
\end{equation}

If W = 2, things are a bit more complicated. The miner will go on a
3-hour excursion, and then be back in its original situation, and thus
have a further expected wait of EY, since ``time starts over.''  In
other words,

\begin{equation}
E(Y|W=2)=3+EY
\end{equation}


Similarly, 

\begin{equation}
E(Y|W=3)=5+EY
\end{equation}

In summary, now considering the \textit{random variable} E(Y$|$W), we have

\begin{equation}
Q=E(Y|W)=\left\{ \begin{array}{rl}
2, & w.p.\frac{1}{3}\\
3+EY, & w.p.\frac{1}{3}\\
5+EY, & w.p.\frac{1}{3}
\end{array}\right. 
\end{equation}

where ``w.p.'' means ``with probability.'' So, using (\ref{itex}) or
(\ref{itexdiscrete}), we have

\begin{equation}
EY=EQ=2\times \frac{1}{3}+(3+EY)\times \frac{1}{3}+(5+EY)\times \frac{1}{3}=\frac{10}{3}+\frac{2}{3}EY
\end{equation}

Equating the extreme left and extreme right ends of this series of equations,
we can solve for EY, which we find to be 10.

It is no accident that the answer, 10, is 2+3+5.  This was discovered by
UCD grad student Ahmed Ahmedin.  Here's why (different from Ahmed's
reasoning):

Let N denote the total number of attempts the miner makes before
escaping (including the successful attempt at the end), and let $U_i$
denote the time spent traveling during the $i^{th}$ attempt, i =
1,...,N.  Then

\begin{eqnarray}
ET &=& E(U_1+...,+U_N) \\ 
&=& E \left [ E(U_1+...,+U_N | N) \right ] 
\end{eqnarray}

Given N, each of $U_1,...,U_{N-1}$ takes on the values 3 and 5, with
probability 0.5 each, while $U_N$ is the constant 2.  Thus

\begin{equation}
E(U_1+...,+U_N | N) = (N-1) \frac{3+5}{2} + 2 = 4N - 2
\end{equation}

N has a geometric distribution with p = 1/3, thus mean 3.  Putting all
this together, we have

\begin{equation}
ET = E(U_1+...,+U_N) = E(4N - 2) = 10
\end{equation}

This would be true if 2, 3 and 5 were replaced by {\it a}, {\it b} and
{\it c}.  In other words, intuitively:  It takes an average of 3 attempts to
escape, with mean time of (a+b+c)/3, so the mean time overall is a+b+c.

It is left to the reader to see how this would change if we assume that the
miner remembers which doors he has already hit.

\subsection{Example:  More on Flipping Coins with Bonuses}

Recall the situation of Section \ref{bonusflip}:  A game involves
flipping a coin k times.  Each time you get a head, you get a bonus
flip, not counted among the k.  (But if you get a head from a bonus
flip, that does not give you its own bonus flip.) Let X denote the
number of heads you get among all flips, bonus or not.   We'll compute
EX.

As before, Y denote the number of heads you obtain through
nonbonus flips.   This is a natural situation in which to try the
Theorem of Total Expectation, conditioning on Y.  Reason as follows:

It would be tempting to say that, given Y = m, X has a binomial
distribution with parameters m and 0.5.  That is not correct, but what
is true is the random variable X-m does have that distribution.   Note
by the way that X-Y is the number of bonus flips.

Then

\begin{eqnarray}
EX &=& E[E(X|Y)] \\ 
&=& E \left [E ( \{ X-Y \} + Y | Y ) \right ] \\
&=& E \left [ 0.5Y + Y \right ] \\
&=& 1.5 EY \\
&=& 0.75 k
\end{eqnarray}

\subsection{Example:  Analysis of Hash Tables}

(Famous example, adapted from various sources.)

Consider a database table consisting of m cells, only some of which are
currently occupied. Each time a new key must be inserted, it is used in
a hash function to find an unoccupied cell. Since multiple keys map to
the same table cell, we may have to probe multiple times before finding
an unoccupied cell.

We wish to find E(Y), where Y is the number of probes needed to insert a
new key.  One approach to doing so would be to condition on W, the
number of currently occupied cells at the time we do a search.  After
finding E(Y$|$W), we can use the Theorem of Total Expectation to find
EY.  We will make two assumptions (to be discussed later):

\begin{itemize}

\item [(a)] Given that W = k, each probe will collide with an existing
cell with probability k/m, with successive probes being independent.

\item [(b)] W is uniformly distributed on the set 1,2,...,m, i.e. P(W =
k) = 1/m for each k.

\end{itemize}

To calculate E(Y$|$W=k), we note that given W = k, then Y is the
number of independent trials until a ``success'' is reached, where
``success'' means that our probe turns out to be to an unoccupied cell.
This is a {\bf geometric} distribution, i.e.

\begin{equation}
P(Y = r | W = k) = {(\frac{k}{m})}^{r-1} (1-\frac{k}{m})
\end{equation}

The mean of this geometric distribution is, from (\ref{eofgeom}),

\begin{equation} 
\frac{1}{1-\frac{k}{m}}
\end{equation}

Then 

\begin{eqnarray}
EY & = & E[E(Y|W)]\\
 & = & \sum ^{m-1}_{k=1}\frac{1}{m}E(Y|W=k)\\
 & = & \sum ^{m-1}_{k=1}\frac{1}{m-k}\\
 & = & 1+\frac{1}{2}+\frac{1}{3}+...+\frac{1}{m-1}\\
 & \approx  & \int_1^m \frac{1}{u} du \\
 & = & ln(m)
\end{eqnarray}

where the approximation is something you might remember from calculus
(you can picture it by drawing rectangles to approximate the area under
the curve.).

Now, what about our assumptions, (a) and (b)?  The assumption in (a) of
each cell having probability k/m should be reasonably accurate if k is
much smaller than m, because hash functions tend to distribute probes
uniformly, and the assumption of independence of successive probes is
all right too, since it is very unlikely that we would hit the same cell
twice.  However, if k is not much smaller than m, the accuracy will
suffer.

Assumption (b) is more subtle, with differing interpretations.  For
example, the model may concern one specific database, in which case the
assumption may be questionable.  Presumably W grows over time, in which
case the assumption would make no sense---it doesn't even {\it have} a
distribution.  We could instead think of a database which grows and
shrinks as time progresses.  However, even here, it would seem that W
would probably oscillate around some value like m/2, rather than being
uniformly distributed as assumed here.  Thus, this model is probably not
very realistic.  However, even idealized models can sometimes provide
important insights.

\section{Simulation of Random Vectors}

Let $X = (X_1,...,X_k)'$ be a random vector having a specified
distribution.  How can we write code to simulate it?  It is not always
easy to do this.  We'll discuss a couple of easy cases here, and
illustrate what one may do in other situations.

The easiest case (and a very frequenly-occurring one) is that in which
the $X_i$ are independent.  One simply simulates them individually, and
that simulates X!

Another easy case is that in which X has a multivariate normal
distribution.  We noted in Section \ref{mvnormdens} that R includes the
function {\bf mvrnorm()}, which we can use to simulate our X here.  The
way this function works is to use the notion of principle components
mentioned in Section \ref{datamining}.  We construct Y = AX for the
matrix A discussed there.  The $Y_i$ are independent, thus easily
simulated, and then we transform back to X via $X = A^{-1}Y$

In general, though, things may not be so easy.  For instance, consider
the distribution in (\ref{tridens}).  There is no formulaic solution
here, but the following strategy works.  

First we find the (marginal) density of X.  As in the case for Y shown
in (\ref{ydens}), we compute

\begin{equation}
f_X(s) = \int_{0}^{s} 8st ~ dt = 4s^3
\end{equation}

Using the method shown in our unit on continuous probability, Section
\ref{genrannumgen}, we can simulate X as

\begin{equation}
X = F_X^{-1}(W)
\end{equation}

where W is a U(0,1) random variable, generated as {\bf runif(1)}.
Since $F_X(u) = u^4$, $F_X^{-1}(v) = v^{0.25}$, and thus our code to
simulate X is

\begin{Verbatim}[fontsize=\relsize{-2}]
runif(1)^0.25
\end{Verbatim}

Now that we have X, we can get Y.  We know that 

\begin{equation}
f_{Y|X}(t|S) = \frac{8st}{4s^3} = \frac{2}{s^2} t
\end{equation}

Remember, s is considered constant.  So again we use the ``inverse-cdf''
method here to find Y, given X, and then we have our pair (X,Y).

\section{Mixture Models}
\label{mix}

To introduce this topic, suppose men's heights are normally distributed
with mean 70 and standard deviation 3, with women's heights being normal
with mean 66 and standard deviation 2.5.  Let H denote the height of a
randomly selected person from the entire population, and let G be the
person's gender, 1 for male and 2 for female.

Then the conditional distribution of H, given G = 1, is N(70,9), and a
similar statement holds for G = 2.  But what about the unconditional
distribution of H?  We can derive it:

\begin{eqnarray}
f_H(t) &=& \frac{d}{dt} F_H(t) \\ 
&=& \frac{d}{dt} P(H \leq t) \\
&=& \frac{d}{dt} P(H \leq t \textrm{ and } G = 1 \textrm{ or }
   H \leq t \textrm{ and } G = 2) \\
&=& \frac{d}{dt} 
   \left [
   0.5 P(H \leq t | G = 1) + 0.5 P(H \leq t | G = 2)
   \right ] \\
&=& \frac{d}{dt} 
\left [ 0.5 F_{H|G=1}(t) + 0.5 F_{H|G=2}(t) \right ] \\
&=& 
0.5 f_{H|G=1}(t) + 0.5 f_{H|G=2}(t) 
\end{eqnarray}

So the density of H in the grand population is the average of the
densities of H in the two subpopulations.  This makes intuitive sense.

In terms of shape, $f_H$, being the average of two bells that are space
apart, will look like a two-humped camel, instead of a bell.  We call
the distribution of H a {\bf mixture distribution}, with the name
alluding to the fact that we mixed the two bells to ge the two-humped
camel.

Another example is that of {\bf overdispersion} in connection with
Poisson models.  Recall the following about the Poisson distribution
family:

\begin{itemize}

\item [(a)] This family is often used to model counts.

\item [(b)] For any Poisson distribution, the variance equals the mean.

\end{itemize}

In some cases in which we are modeling count data, one may try to fit a
mixture of several Poisson distributions, instead of a single one.  This
frees us of constraint (b), as can be seen as follows:

Suppose X can equal 1,2,...,k, with probabilities $p_1,...,p_k$ that sum
to 1.  Say the distribution of Y given X = i is Poisson with parameter
$\lambda_i$.  Then by the Law of Total Expectation,

\begin{eqnarray}
\label{meanlamb}
EY &=& E[E(|X)] \\ 
&=& E(\lambda_X) \\
&=& \sum_{i=1}^k p_i \lambda_i
\end{eqnarray}

Note that in the above, the expression $\lambda_X$ is a random variable,
since its subscript X is random.  Indeed, it is a function of X, so
Equation (\ref{egofx}) then applies, yielding the final equation.  The
random variable $\lambda_x$ takes on the values $\lambda_1,...,\lambda_k$
with probabilities $p_1,...,p_k$, hence that final sum.

The corresponding formula for variance, (\ref{bis}), can be used to
derive Var(Y).

\begin{eqnarray}
Var(Y) &=& E[Var(Y|X)] + Var[E(Y|X)] \\ 
&=& E(\lambda_X) + Var(\lambda_X)
\end{eqnarray}

We already evaluated the first term, in (\ref{meanlamb}).  The second
term is evaluated the same way:  This is the variance of a random
variable that takes on the values $\lambda_1,...,\lambda_k$
with probabilities $p_1,...,p_k$, which is

\begin{equation}
\sum_{i=1}^k p_i (\lambda_i - \overline{\lambda})^2
\end{equation}

where 

\begin{equation}
\overline{\lambda} =  E\lambda_X = \sum_{i=1}^k p_i \lambda_i
\end{equation}

Thus

\begin{equation}
EY = \overline{\lambda}
\end{equation}

and

\begin{equation}
Var(Y) = \overline{\lambda} + 
\sum_{i=1}^k p_i (\lambda_i - \overline{\lambda})^2
\end{equation}

So, as long as the $\lambda_i$ are not equal and no $p_i = 1$, we have

\begin{equation}
Var(Y) > EY
\end{equation}

in this Poisson mixture model, in contrast to the single-Poisson case
in which Var(Y) = EY.  You can now see why the Poisson mixture model is
called an overdispersion model.

So, if one has count data in which the variance is greater than the
mean, one might try using this model.

In mixing the Poissons, there is no need to restrict to discrete X.  In
fact, it is not hard to derive the fact that if X has a gamma
distribution with parameters r and p/(1-p) for some $0 < p < 1$, and Y
given X has a Poisson distribution with mean X, then the resulting Y
neatly turns out to have a negative binomial distribution.

\section{Transform Methods}

We often use the idea of \textbf{transform} functions.  For example, you
may have seen \textbf{Laplace transforms} in a math or engineering
course.  The functions we will see here differ from this by just
a change of variable. 

Though in the form used here they involve only univariate distributions,
their applications are often multivariate, as will be the case here.

\subsection{Generating Functions}

Let's start with the \textbf{generating function}.
For any nonnegative-integer valued
random variable V, its generating function is defined by

\begin{equation}
\label{genftn}
g_V(s)=E(s^{V}) = \sum_{i=0}^\infty s^i p_V(i), ~ 0 \leq s \leq 1
\end{equation}

For instance, suppose N has a geometric distribution with parameter p,
so that $p_N(i) = (1-p) p^{i-1}$, i = 1,2,...  Then

\begin{equation}
g_N(s) = \sum_{i=1}^\infty s^i \cdot (1-p) p^{i-1} 
= \frac{1-p}{p} \sum_{i=1}^\infty s^i \cdot p^i 
= \frac{1-p}{p} \frac{ps}{1-ps}
= \frac{(1-p)s}{1-ps}
\end{equation}

Why restrict s to the interval [0,1]?  The answer is that for $s > 1$
the series in (\ref{genftn}) may not converge.  for $0 \leq s \leq 1$,
the series does converge.  To see this, note that if s = 1, we just get
the sum of all probabilities, which is 1.0.  If a nonnegative s is less
than 1, then $s^i$ will also be less than 1, so we still have
convergence.

One use of the generating function is, as its name implies, to generate
the probabilities of values for the random variable in question.  In
other words, if you have the generating function but not the
probabilities, you can obtain the probabilities from the function.
Here's why:  For clarify, write (\ref{genftn}) as

\begin{equation}
g_V(s) = P(V=0) + s P(V=1) + s^2 P(V=2) + ...  
\end{equation}

From this we see that 

\begin{equation}
g_V(0) = P(V = 0)
\end{equation}

So, we can obtain P(V = 0) from the generating function.  Now
differentiating (\ref{genftn}) with respect to s, we have

\begin{eqnarray}
g'_V(s) &=&
\frac{d}{ds} \left [ P(V=0) + s P(V=1) + s^2 P(V=2) + ...  \right ]
\nonumber \\
&=&
P(V=1) + 2s P(V=2) + ...  
\end{eqnarray}

So, we can obtain P(V = 2) from $g'_V(0)$, and in a similar manner can
calculate the other probabilities from the higher derivatives.

\subsection{Moment Generating Functions}

The generating function is handy, but it is limited to discrete random
variables.  More generally, we can use the {\bf moment generating
function}, defined for any random variable X as

\begin{equation}
\label{momgen}
m_X(t) = E[e^{tX}] 
\end{equation}

for any t for which the expected value exists.

That last restriction is anathema to mathematicians, so they use the
characteristic function, 

\begin{equation}
\phi_X(t) = E[e^{itX}] 
\end{equation}

which exists for any t.  However, it makes use of pesky complex numbers,
so we'll stay clear of it here.

Differentiating (\ref{momgen}) with respect to t, we have

\begin{equation}
m_X'(t) = E[X e^{tX}] 
\end{equation}

We see then that

\begin{equation}
m_X'(0) = EX
\end{equation}

So, if we just know the moment-generating function of X, we can obtain
EX from it.  Also,

\begin{equation}
m_X''(t) = E(X^2 e^{tX})
\end{equation}

so

\begin{equation}
m_X''(0) = E(X^2)
\end{equation}

In this manner, we can for various k obtain $E(X^k)$, the {\bf k$th$
moment} of X, hence the name.

\subsection{Transforms of Sums of Independent Random Variables}
\label{tranfssums}

Suppose X and Y are independent and their moment generating functions
are defined.  Let Z = X+Y.  then

\begin{equation}
m_Z(t) = E[e^{t(X+Y)}] = E[e^{tX} e^{tY}] = E(e^{tX}) \cdot E(e^{tY}) =
m_X(t) m_Y(t)
\end{equation}

In other words, the mgf of the sum is the product of the mgfs!  This is
true for other transforms, by the same reasoning.

Similarly, it's clear that the mgf of a sum of three independent
variables is again the product of their mgfs, and so on.

\subsection{Example:  Network Packets}

As an example, suppose say the number of packets N received on a network
link in a given time period has a Poisson distribution with mean $\mu$,
i.e.

\begin{equation}
P(N = k) = \frac{e^{-\mu} \mu^k}{k!}, k = 0,1,2,3,...  
\end{equation}

\subsubsection{Poisson Generating Function}

Let's first find its generating function.

\begin{equation}
g_N(t) = \sum_{k=0}^{\infty} t^k  \frac{e^{-\mu} \mu^k}{k!} 
= e^{-\mu} \sum_{k=0}^{\infty} \frac{(\mu t)^k}{k!}
= e^{-\mu+\mu t}
\end{equation}

where we made use of the Taylor series from calculus,

\begin{equation}
e^u =  \sum_{k=0}^{\infty} u^k/k!
\end{equation}

\subsubsection{Sums of Independent Poisson Random Variables Are Poisson
Distributed}
\label{sumpoi}

Supposed packets come in to a network node from two independent links,
with counts $N_1$ and $N_2$, Poisson distributed with means $\mu_1$ and
$\mu_2$.  Let's find the distribution of $N = N_1+N_2$, using a
transform approach.

From Section \ref{tranfssums}:

\begin{equation}
\label{sumpoieqn}
g_N(t) = g_{N_1}(t) g_{N_2}(t) = e^{-\nu + \nu t}
\end{equation}

where $\nu = \mu_1 + \mu_2$.

But the last expression in (\ref{sumpoieqn}) is the generating function for
a Poisson distribution too!  And since there is a one-to-one
correspondence between distributions and transforms, we can conclude
that N has a Poisson distribution with parameter $\nu$.  We of
course knew that N would have mean $\nu$ but did not know that N would
have a Poisson distribution.

So:  A sum of two independent Poisson variables itself has a Poisson
distribution.  By induction, this is also true for sums of k independent
Poisson variables.

\subsection{Random Number of Bits in Packets on One Link}
\label{randomn}

Consider just one of the two links now, and for convenience denote the
number of packets on the link by N, and its mean as $\mu$.  Continue to
assume that N has a Poisson distribution.

Let B denote the number of bits in a packet, with $B_1,...,B_N$ denoting
the bit counts in the N packets.  We assume the $B_i$ are independent
and identically distributed. The total number of bits received during
that time period is

\begin{equation}
T=B_1+...+B_N
\end{equation}

Suppose the generating function of B is known to be h(s). Then what is
the generating function of T?

\begin{eqnarray}
g_T(s) & = & E(s^{T})\\
 & = & E[E(s^{T}|N)]\\
 & = & E[E(s^{B_1+...+B_N}|N)]\\
 & = & E[E(s^{B_1}|N)...E(s^{B_N}|N)]\\
 & = & E[h(s)^{N}]\\
 & = & g_N[h(s)]\\
 & = & e^{-\mu+\mu h(s)}
\end{eqnarray}  

Here is how these steps were made: 

\begin{itemize}

\item From the first line to the second, we used
the Theorem of Total Expectation. 

\item From the second to the third, we just used the definition of T.

\item From the third to the fourth lines, we have used algebra plus the
fact that the expected value of a product of independent random
variables is the product of their individual expected
values.

\item From the fourth to the fifth, we used the definition of h(s).

\item From the fifth to the sixth, we used the definition of $g_N$.

\item From the sixth to the last we used the formula for the generating
function for a Poisson distribution with mean $\mu$.

\end{itemize}

We can then get all the information about T we need from this formula, such
as its mean, variance, probabilities and so on, as seen previously. 

\subsection{Other Uses of Transforms}

Transform techniques are used heavily in queuing analysis, including for
models of computer networks.  The techniques are also used extensively
in modeling of hardware and software reliability.

Transforms also play key roles in much of theoretical probability, the
Central Limit Theorems\footnote{The plural is used here because there
are many different versions, which for instance relax the condition that
the summands be independent and identically distributed.} being a good
example.  Here's an outline of the proof of the basic CLT, assuming the
notation of Section \ref{formalclt}:

First rewrite Z as

\begin{equation}
Z = \sum_{i=1}^n \frac{X_i - m}{v \sqrt{n}}
\end{equation}

Then work with the characteristic function of Z:

\begin{eqnarray}
c_Z(t) &=& E(e^{itZ}) ~~~~ \textrm{(def.)} \\ 
&=& \Pi_{i=1}^n E[e^{it(X_i-m)/(v\sqrt{n})}] ~~~~ \textrm{(indep.)} \\
&=&  \Pi_{i=1}^n E[e^{it(X_1-m)/(v\sqrt{n})}] ~~~~ \textrm{(ident. distr.)} \\
&=& [g(\frac{it}{\sqrt{n}})]^n
\label{gpower}
\end{eqnarray}

where g(s) is the characteristic function of $(X_1-m)/v$, i.e.

\begin{equation}
g(s) = E[e^{is \cdot \frac{X_1 - m}{v}}]
\end{equation}

Now expand (\ref{gpower}) in a Taylor series around 0, and use the fact
that $g'(0)$ is the expected value of $(X_1-m)/v$, which is 0:

\begin{eqnarray}
[g(\frac{t}{\sqrt{n}})]^n 
&=& \left [1 - \frac{t^2}{2n} + o(\frac{t^2}{n}) \right ]^n \\
&\rightarrow& e^{-t^2/2} \textrm{ as } n \rightarrow \infty
\label{ourcltgoal}
\end{eqnarray}

where we've also used the famous fact that $(1 - s/n)^n$ converges to
$e^{-s}$ as $n \rightarrow \infty$.

But (\ref{ourcltgoal}) is the 

\section{Vector Space Interpretations (for the mathematically
adventurous only)}
\label{adventure}

The abstract vector space notion in linear algebra has many applications
to statistics.  We develop some of that material in this section.

Consider the set of all random variables associated with some
``experiment,'' in our ``notebook'' sense from Section \ref{repeatexpt}.
(In more mathematical treatments, we would refer here to the set of all
random variables defined on some {\bf probability space}.)  Note that
some of these random variables are independent of each other, while
others are not; we are simply considering the totality of all random
variables that arise from our experiment.

Let $\cal V$ be the set of all such random variables having finite
variance and mean 0.  We can set up $\cal$ as a vector space.  For that,
we need to define a sum and a scalar product.  Define the sum of any two
vectors X and Y to be the random variable X+Y.  For any constant c, the
vector cX is the random variable cX.  Note that $\cal V$ is closed under
these operations, as it must be:  If X and Y both have mean 0, then X+Y
does too, and so on.

Define an inner product on this space:

\begin{equation}
(X,Y) = E(XY) = Cov(X,Y)
\end{equation}

(Recall that Cov(X,Y) = E(XY) - EX EY, and that we are working with
random variables that have mean 0.) Thus the norm of a vector X is

\begin{equation}
||X|| = {(X,X)}^{0.5} = \sqrt{E(X^2)} = \sqrt{Var(X)}
\end{equation}

again since E(X) = 0.

\section{Properties of Correlation}
\label{propcorr}

The famous Cauchy-Schwarz Inequality for inner products says,

\begin{equation}
|(X,Y)| \leq ||X|| ~ ||Y||
\end{equation}

i.e.

\begin{equation}
|\rho(X,Y)| \leq 1
\end{equation}

Also, the Cauchy-Schwarz Inequality yields equality if and only if one
vector is a scalar multiple of the other, i.e. Y = cX for some c.
When we then translate this to random variables of nonzero means,
we get Y = cX + d.  

In other words, the correlation between two random variables is between
-1 and 1, with equality if and only if one is an exact linear function
of the other.

\section{Conditional Expectation As a Projection}
\label{elegant} 

For a random variable X in $\cal$, let $\cal W$ denote the subspace of
$\cal V$ consisting of all functions h(X) with mean 0 and finite
variance.  (Again, note that this subspace is indeed closed under vector
addition and scalar multiplication.) 

Now consider any Y in $\cal V$.  Recall that the {\it projection} of Y
onto $\cal W$ is the closest vector T in $\cal W$ to Y, i.e. T minimizes
$||Y-T||$.  That latter quantity is 

\begin{equation}
\label{l2}
{\left ( E[{(Y-T)}^2] \right )}^{0.5}
\end{equation}

To find the minimizing T, consider first the minimization of

\begin{equation}
\label{minsc}
E[{(S-c)}^2]
\end{equation}

with respect to constant c for some random variable S.  We already
solved this problem back in Section \ref{gofc}.  The minimizing value 
is c = ES.

Getting back to (\ref{l2}), use the Law of Total Expectation to write

\begin{equation}
\label{min2}
E[{(Y-T)}^2] = E \left (  E[{(Y-T)}^2|X]\right )
\end{equation}

From what we learned with (\ref{minsc}), applied to the conditional
(i.e. inner) expectation in (\ref{min2}), we see that the T which
minimizes (\ref{min2}) is T = E(Y$|$X).  

In other words, the conditional mean is a projection!  Nice, but is this
useful in any way?  The answer is yes, in the sense that it guides the
intuition.  All this is related to issues of statistical
prediction---here we would be predicting Y from X---and the geometry
here can really guide our insight.  This is not very evident without
getting deeply into the prediction issue, but let's explore some of the
implications of the geometry.

For example, a projection is perpendicular to the line connecting the
projection to the original vector.  So

\begin{equation}
\label{err}
0 = (E(Y|X),Y-E(Y|X)) 
% = E \left [ E(Y|X) \left ( Y-E(Y|X) \right ) \right ]
= Cov[E(Y|X), ~ Y-E(Y|X)]
\end{equation}

This says that the prediction E(Y$|$X) is uncorrelated with the
prediction error, Y-E(Y$|X$).  This in turn has statistical importance.
Of course, (\ref{err}) could have been derived directly, but the
geometry of the vector space intepretation is what suggested we look at
the quantity in the first place.  Again, the point is that the vector
space view can guide our intuition.

Simlarly, the Pythagorean Theorem holds, so

\begin{equation}
\label{py}
{||Y||}^2 = {||E(Y|X)||}^2 + {||Y-E(Y|X)||}^2
\end{equation}

which means that

\begin{equation}
\label{py2}
Var(Y) = Var[E(Y|X)] + Var[Y-E(Y|X)]
\end{equation}

Equation (\ref{py2}) is a common theme in linear models in statistics,
the decomposition of variance.  

There is an equivalent form that is useful as well, derived as follows
from the second term in (\ref{py2}).  Since

\begin{equation}
E[Y-E(Y|X)] = EY - E[E(Y|X)] = EY - EY = 0
\end{equation}

we have

\begin{eqnarray}
\label{verycomplex}
Var[Y-E(Y|X)] &=& E \left [ (Y-E(Y|X))^2 \right ] \\ 
&=& E \left [ Y^2 -2YE(Y|X) + (E(Y|X))^2 \right ] 
\end{eqnarray}

Now consider the middle term, $E[-2YE(Y|X)]$.  Conditioning on X and
using the Law of Total Expectation, we have

\begin{equation}
E[-2YE(Y|X)] = -2 E \left [ (E(Y|X))^2 \right ]
\end{equation}

Then (\ref{verycomplex}) becomes

\begin{eqnarray}
Var[Y-E(Y|X)] &=& E (Y^2) - E \left [ (E(Y|X))^2 \right ] \\
&=& E \left [ E(Y^2 | X) \right ] - E \left [ (E(Y|X))^2 \right ] \\
&=& E \left ( E(Y^2 | X) - (E(Y|X))^2 \right ) \\
&=& E \left [ Var(Y|X) \right ]
\end{eqnarray}

the latter coming from our old friend, $Var(U) = E(U^2) - (EU)^2$, with
U being Y here, under conditioning by X.

In other words, we have just derived another famous formula:

\begin{equation}
Var(Y) = E[Var(Y|X)] + Var[E(Y|X)]
\end{equation}

\section{Proof of the Law of Total Expectation}
\label{proveitex} 

Let's prove (\ref{itex}) for the case in which W and Y take values only
in the set \{1,2,3,...\}.  Recall that if T is an integer-value random
variable and we have some function h(), then L = h(T) is another random
variable, and its expected value can be calculated as\footnote{This is
sometimes called The Law of the Unconscious Statistician, by nasty
probability theorists who look down on statisticians.  Their point is
that technically $EL = \sum_k k P(L = k)$, and that (\ref{unconscious})
must be proven, whereas the statisticians supposedly think it's a
definition.}  

\begin{equation}
\label{unconscious}
E(L) = \sum_k h(k) P(T = k)
\end{equation}

In our case here, Q is a function of W, so we find its expectation from
the distribution of W:

\begin{eqnarray*}
E(Q) & = & \sum ^{\infty }_{i=1}g(i) P(W=i)\\
 & = & \sum ^{\infty }_{i=1}E(Y|W=i)P(W=i)\\
 & = & \sum ^{\infty }_{i=1} \left [ \sum ^{\infty }_{j=1}jP(Y=j|W=i) \right ] P(W=i) \\
 & = & \sum ^{\infty }_{j=1}j\sum ^{\infty }_{i=1}P(Y=j|W=i)P(W=i)\\
 & = & \sum ^{\infty }_{j=1}jP(Y=j)\\
 & = & E(Y)
\end{eqnarray*}
 In other words, 

\begin{equation}
E(Y)=E[E(Y|W)]
\end{equation}

\startproblemset

\oneproblem
In the catchup game in Section \ref{catchupgame}, let $V$ and $W$ denote
the winnings of the two players after only one turn.  Find $P(V > 0.4)$.

\oneproblem
Use transform methods to derive some properties of the Poisson family:

\begin{itemize}

\item [(a)] Show that for any Poisson random variable, its mean and
variance are equal.

\item [(b)] Suppose $X$ and $Y$ are independent random variables, each
having a Poisson distribution.  Show that $Z = X+Y$ again has a Poisson
distribution.

\end{itemize}

\oneproblem Suppose one keeps rolling a die. Let $S_n$ denote the total
number of dots after n rolls, mod 8, and let $T$ be the number of rolls
needed for the event $S_n = 0$ to occur. Find $E(T)$, using an approach
like that in the ``trapped miner'' example in Section
\ref{trappedminer}.

\oneproblem
In our ordinary coins which we use every day, each one has a slightly 
different probability of heads, which we'll call $H$.  Say $H$ has 
the distribution $N(0.5, 0.03^2)$.  We choose a coin from a batch at 
random, then toss it 10 times.  Let $N$ be the number of heads we get.  
Find $Var(N)$.


\oneproblem
Suppose the number N of bugs in a certain number of lines of code has a
Poisson distribution, with parameter L, where L varies from one
programmer to another. Show that Var(N) = EL + Var(L).

\oneproblem
This problem arises from the analysis of random graphs, which for
concreteness we will treat here as social networks such as Facebook.

In the model here, each vertex in the graph has N friends, N being a
random variable with the same distribution at every vertex. One thinks
of each vertex as generating its links, unterminated, i.e. not tied yet
to a second vertex. Then the unterminated links of a vertex pair off at
random with those of other vertices. (Those that fail will just pair in
self loops, but we'll ignore that.)

Let M denote the number of friends a friend of mine has. That is, start
at a vertex A, and follow a link from A to another vertex, say B. M is
the number of friends B has (we'll include A in this number).

\begin{itemize}

\item [(a)] Since an unterminated link from A is more likely to pair up
with a vertex that has a lot of links, a key assumption is that P(M = k)
= ck P(N = k) for some constant c. Fill in the blank: This is an example
of the setting we studied called \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_.

\item [(b)] Show the following relation of generating functions: $g_M(s)
= g_N'(s)/EN$. 

\end{itemize}

\oneproblem
Suppose Type 1 batteries have exponentially distributed lifetimes with
mean 2.0 hours, while Type 2 battery lifetimes are exponentially
distributed with mean 1.5.  We have a large box containing a mixture of
the two types of batteries, in proportions q and 1-q.  We reach into the
box, choose a battery at random, then use it.  Let $Y$ be the lifetime
of the battery we choose.  Use the Law of Total Variance, (\ref{bis}),
to find $Var(Y)$.


\oneproblem
In the backup battery example in Section \ref{backup}, find Var(W),
using the Law of Total Expectation.

\oneproblem
Let X denote the number we obtain when we roll a single die
once.  Let $G_X(s)$ denote the generating function of X.

\begin{itemize}

\item [(a)] Find $G_X(s)$.

\item [(b)] Suppose we roll the die 5 times, and let T denote the
total number of dots we get from the 5 rolls.  Find $G_T(s)$.

\end{itemize}

\oneproblem
Consider this model of disk seeks. For simplicity, we'll assume a very
tiny number of tracks, 3. Let $X_1$ and $X_2$ denote the track numbers
of two successive disk requests. Each has a uniform distribution on
\{1,2,3\}. But given $X_1 = i$, then $X_2 = i$ with probability 0.4, with
$X_2$ being j with probability 0.3, $j \neq i$. (Convince yourself that
these last two sentences are consistent with each other.) Find the
following:

\begin{itemize}

\item [(a)] $P(|X_1 - X_2| \leq 1)$

\item [(b)] $E(|X_1 - X_2|)$

\item [(c)] $F_{X_1,X_2}(2,2)$

\end{itemize}

\oneproblem
Consider the computer worm example in Section \ref{senthi}.  Let R
denote the time it takes to go from state 1 to state 3.  Find $f_R(v)$.  
(Leave your answer in integral form.)

\oneproblem
Suppose (X,Y) has a bivariate normal distribution, with EX = EY = 0,
Var(X) = Var(Y) = 1, and $\rho(X,Y) = 0.2$.  Find the following, in
integral forms:

\begin{itemize}

\item [(a)] $E(X^2+XY^{0.5})$

\item [(b)] $P(Y > 0.5X)$

\item [(c)] $F_{X,Y}(0.6,0.2)$

\end{itemize}
                      
\oneproblem
Suppose $X_i$, i = 1,2,3,4,5 are independent and each have mean 0 and   
variance 1.  Let $Y_i = X_{i+1} - X_i$, i = 1,2,3,4.  Using the 
material in Section \ref{matrix}, find the covariance matrix of 
$Y = (Y_1, Y_2, Y_3, Y_4)$.

% Plot of 2-D normal, adapted from R Graph Gallery,
% http://addictedtor.free.fr/graphiques/graphcode.php?graph=42:
% 
% mu1 <- 0  # EX1
% mu2 <- 0  # EX2
% s11 <- 10  # Var(X1) = 10
% s22 <- 10  # Var(X2) = 15
% x1 <- seq(-10,10,length=41) 
% x2 <- x1 
% 
% # bivariate normal density
% f <- function(x1,x2,rho){
%         term1 <- 1/(2*pi*sqrt(s11*s22*(1-rho^2)))                               
%         term2 <- -1/(2*(1-rho^2))                                               
%         term3 <- (x1-mu1)^2/s11                                                 
%         term4 <- (x2-mu2)^2/s22                                                 
%         term5 <- 2*rho*((x1-mu1)*(x2-mu2))/(sqrt(s11)*sqrt(s22))               
%         term1*exp(term2*(term3+term4-term5))                                    
% } 
% 
% plot2dnorm <- function(x1,x2,f,rho) {
%    z <- outer(x1,x2,f,rho)
%    persp(x1, x2, z,
%       col="lightgreen",                                                         
%       theta=30, phi=20,                                                         
%       r=50,                                                                     
%       d=0.1,                                                                    
%       expand=0.5,                                                               
%       ltheta=90, lphi=180,                                                      
%       shade=0.75,                                                               
%       ticktype="detailed",                                                      
%       nticks=5) 
% }
% 
% 
% 
% # simulation of dice game; NOT efficient R code
% 
% # game:  roll a die 50 times; each time, get $5 for a 1, $2 for a 2 or 3
% 
% # random multinomial generator (R has one too)
% rmn <- function(pcdf) {  # pcdf is the cdf vector of the multinomial
%    u <- runif(1)
%    ncats <- length(pcdf)
%    for (i in 1:(ncats-1))
%       if (u <= pcdf[i]) return(i)
%    return(ncats)
% }
% 
% # do the simulation
% nplays <- 50
% nreps <- 1000
% p1 <- 1/6
% p23 <- 3/6
% prest <- 1
% nums <- matrix(nrow=nreps,ncol=2)
% wins <- vector(length=nreps)
% for (rep in 1:nreps) {
%    totwin <- 0
%    numones <- 0
%    numtwosthrees <- 0
%    for (i in 1:nplays) {
%       r <- rmn(c(p1,p23,prest))
%       if (r == 1) numones <- numones + 1
%       else if (r == 2) numtwosthrees <- numtwosthrees + 1
%    }
%    nums[rep,] <- c(numones,numtwosthrees)
%    wins[rep] <- 5*numones + 2*numtwosthrees
% }
% 
% # analyze the data
% # find F(,)
% up1 <- 12
% up2 <- 16
% print(nrow(nums[nums[,1] <= up1 & nums[,2] <= up2,])/nreps)
% # winnings should be approx normal distributed
% hist(wins)
% # find P(win > $90)
% print(length(wins[wins>90])/nreps)
% 
% 
% # plotting the double integral
% plot(c(0,1),c(0,1),type="n",xlab="s",ylab="t")
% polygon(c(0,1,1,0),c(0,0,1,0),col="gray")
% polygon(c(1,1,0.5,1),c(0,1,0.5,0),density=5)
% lines(c(0.7,0.7),c(0.3,0.7),lwd=3)
% text(locator(1),"t=1-s")
% locator(1)
% text(locator(1),"t=s")

